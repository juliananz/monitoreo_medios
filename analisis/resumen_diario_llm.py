"""
Daily executive summary generated by local LLM (Ollama).

Primary data source: aggregation tables (agregacion_diaria, etc.).
Supplementary: limited highlighted news (max 3) from noticias for illustrative URLs.
Generates a concise Spanish summary using llama3.1:8b-instruct-q4_0.
"""

import json
import logging
import requests
from datetime import date, timedelta
from pathlib import Path
from typing import Dict, List, Optional

from analisis.utils import get_db_connection

logger = logging.getLogger(__name__)

# Configuration
# Requires: ollama serve (not ollama run)
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.1:8b-instruct-q4_0"
OUTPUT_DIR = Path(__file__).resolve().parents[1] / "output"


# =============================================================================
# DATA EXTRACTION (from aggregation tables only)
# =============================================================================

def get_metricas_globales(fecha: date) -> Optional[Dict]:
    """Get global metrics for a specific date from agregacion_diaria."""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                total_noticias,
                total_relevantes,
                total_riesgo,
                total_oportunidad,
                total_mixto,
                medios_activos
            FROM agregacion_diaria
            WHERE fecha = ?
        """, (fecha.isoformat(),))
        row = cursor.fetchone()

        if not row:
            return None

        return {
            "total_noticias": row[0] or 0,
            "total_relevantes": row[1] or 0,
            "total_riesgo": row[2] or 0,
            "total_oportunidad": row[3] or 0,
            "total_mixto": row[4] or 0,
            "medios_activos": row[5] or 0
        }


def get_cambio_vs_ayer(fecha: date) -> Dict:
    """Calculate day-over-day changes from agregacion_diaria."""
    ayer = fecha - timedelta(days=1)

    hoy = get_metricas_globales(fecha)
    anterior = get_metricas_globales(ayer)

    if not hoy or not anterior:
        return {}

    def calc_delta(actual, previo):
        if previo == 0:
            return None
        return round((actual - previo) / previo * 100, 1)

    return {
        "noticias": calc_delta(hoy["total_noticias"], anterior["total_noticias"]),
        "riesgo": calc_delta(hoy["total_riesgo"], anterior["total_riesgo"]),
        "oportunidad": calc_delta(hoy["total_oportunidad"], anterior["total_oportunidad"])
    }


def get_top_temas(fecha: date, limit: int = 3) -> List[Dict]:
    """Get top topics by news count for a specific date."""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                t.nombre as tema,
                atd.total_noticias,
                atd.total_riesgo,
                atd.total_oportunidad
            FROM agregacion_tema_diaria atd
            JOIN temas t ON atd.tema_id = t.id
            WHERE atd.fecha = ?
            ORDER BY atd.total_noticias DESC
            LIMIT ?
        """, (fecha.isoformat(), limit))

        return [
            {
                "tema": row[0],
                "noticias": row[1] or 0,
                "riesgo": row[2] or 0,
                "oportunidad": row[3] or 0
            }
            for row in cursor.fetchall()
        ]


def get_top_regiones(fecha: date, limit: int = 3) -> List[Dict]:
    """Get top regions by news count for a specific date."""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                CASE
                    WHEN ard.region_id = -1 THEN ard.nivel_geografico
                    ELSE COALESCE(r.nombre, ard.nivel_geografico)
                END as region,
                ard.nivel_geografico,
                ard.total_noticias,
                ard.total_riesgo
            FROM agregacion_region_diaria ard
            LEFT JOIN regiones r ON ard.region_id = r.id AND ard.region_id != -1
            WHERE ard.fecha = ?
            ORDER BY ard.total_noticias DESC
            LIMIT ?
        """, (fecha.isoformat(), limit))

        return [
            {
                "region": row[0],
                "nivel": row[1],
                "noticias": row[2] or 0,
                "riesgo": row[3] or 0
            }
            for row in cursor.fetchall()
        ]


def get_top_entidades(fecha: date, limit: int = 5) -> List[Dict]:
    """Get top entities by mentions for a specific date."""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                e.nombre_canonico as entidad,
                e.tipo,
                aed.menciones,
                aed.noticias_riesgo
            FROM agregacion_entidad_diaria aed
            JOIN entidades e ON aed.entidad_id = e.id
            WHERE aed.fecha = ?
            ORDER BY aed.menciones DESC
            LIMIT ?
        """, (fecha.isoformat(), limit))

        return [
            {
                "entidad": row[0],
                "tipo": row[1],
                "menciones": row[2] or 0,
                "contexto_riesgo": row[3] or 0
            }
            for row in cursor.fetchall()
        ]


def get_highlighted_news(fecha: date, limit: int = 3) -> List[Dict]:
    """
    Get highlighted news items for illustrative purposes.

    Returns top relevant news ordered by score and date.
    This is supplementary to aggregation data, not a replacement.
    """
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                titulo,
                medio,
                temas,
                riesgo,
                oportunidad,
                nivel_geografico,
                url
            FROM noticias
            WHERE relevante = 1
              AND fecha = ?
            ORDER BY score DESC, fecha DESC
            LIMIT ?
        """, (fecha.isoformat(), limit))

        def clasificar_tipo(riesgo, oportunidad):
            if riesgo and oportunidad:
                return "mixto"
            elif riesgo:
                return "riesgo"
            elif oportunidad:
                return "oportunidad"
            return "neutro"

        return [
            {
                "titulo": row[0],
                "medio": row[1],
                "tema": row[2] or "Sin tema",
                "tipo": clasificar_tipo(row[3], row[4]),
                "region": row[5] or "Nacional",
                "url": row[6] or ""
            }
            for row in cursor.fetchall()
        ]


# =============================================================================
# PAYLOAD CONSTRUCTION
# =============================================================================

def construir_payload(fecha: date) -> Optional[Dict]:
    """Build the JSON payload for the LLM from aggregation data."""
    metricas = get_metricas_globales(fecha)

    if not metricas:
        logger.warning(f"No aggregation data found for {fecha.isoformat()}")
        return None

    payload = {
        "fecha": fecha.isoformat(),
        "metricas_globales": metricas,
        "cambio_vs_ayer": get_cambio_vs_ayer(fecha),
        "top_temas": get_top_temas(fecha, limit=3),
        "top_regiones": get_top_regiones(fecha, limit=3),
        "top_entidades": get_top_entidades(fecha, limit=5),
        "noticias_destacadas": get_highlighted_news(fecha, limit=3)
    }

    return payload


# =============================================================================
# LLM INTERACTION
# =============================================================================

PROMPT_TEMPLATE = """Eres un analista de inteligencia de medios. Genera un resumen ejecutivo en español basado EXCLUSIVAMENTE en los datos proporcionados.

DATOS DEL DIA:
{json_payload}

INSTRUCCIONES:
1. Parrafo 1: vision general del dia (volumen de noticias y cambios respecto al dia anterior).
2. Parrafo 2: temas principales y su balance riesgo/oportunidad.
3. Parrafo 3: entidades clave y concentracion geografica.
4. Parrafo 4: referencia explicita a 2-3 noticias destacadas usando su titulo, medio y contexto.
   - Para cada noticia destacada, incluye el enlace con el formato exacto:
     "Para leer la nota completa entrar a: <URL> (requiere iniciar sesion)"

REGLAS:
- No inventes informacion que no este en los datos.
- No inferieras causas ni motivaciones.
- Usa numeros y porcentajes concretos cuando esten disponibles.
- Mantén un tono neutral e institucional.
- Maximo 250 palabras.
- No uses markdown ni listas, solo prosa.

RESUMEN:
"""


def generar_resumen_llm(payload: Dict) -> Optional[str]:
    """Call local Ollama LLM to generate the summary."""
    prompt = PROMPT_TEMPLATE.format(json_payload=json.dumps(payload, indent=2, ensure_ascii=False))

    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 600
                }
            },
            timeout=600
        )
        response.raise_for_status()

        result = response.json()
        return result.get("response", "").strip()

    except requests.exceptions.ConnectionError:
        logger.error("Cannot connect to Ollama. Is it running? (ollama serve)")
        return None
    except requests.exceptions.Timeout:
        logger.error("Ollama request timed out")
        return None
    except Exception as e:
        logger.error(f"Error calling Ollama: {e}")
        return None


# =============================================================================
# OUTPUT
# =============================================================================

def guardar_resumen(fecha: date, resumen: str) -> Path:
    """Save the generated summary to a text file."""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    filename = f"resumen_llm_{fecha.isoformat()}.txt"
    filepath = OUTPUT_DIR / filename

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"RESUMEN EJECUTIVO - {fecha.isoformat()}\n")
        f.write("=" * 50 + "\n\n")
        f.write(resumen)
        f.write("\n")

    return filepath


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================

def generar_resumen_diario_llm(fecha: Optional[date] = None) -> Optional[Path]:
    """
    Generate daily executive summary using local LLM.

    Args:
        fecha: Date to summarize. Defaults to today.

    Returns:
        Path to the generated summary file, or None if failed.
    """
    if fecha is None:
        fecha = date.today()

    logger.info(f"Generating LLM summary for {fecha.isoformat()}...")

    # Build payload from aggregation tables
    payload = construir_payload(fecha)
    if not payload:
        logger.error("Failed to build payload - no data available")
        return None

    logger.debug(f"Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")

    # Generate summary via LLM
    resumen = generar_resumen_llm(payload)
    if not resumen:
        logger.error("Failed to generate summary from LLM")
        return None

    # Save to file
    filepath = guardar_resumen(fecha, resumen)
    logger.info(f"Summary saved to: {filepath}")

    return filepath


# =============================================================================
# CLI
# =============================================================================

if __name__ == "__main__":
    import sys

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # Parse optional date argument
    if len(sys.argv) > 1:
        try:
            target_date = date.fromisoformat(sys.argv[1])
        except ValueError:
            print(f"Invalid date format: {sys.argv[1]}. Use YYYY-MM-DD")
            sys.exit(1)
    else:
        target_date = date.today()

    # Generate summary
    result = generar_resumen_diario_llm(target_date)

    if result:
        print(f"\nSummary generated: {result}")
        print("\nContent:")
        print("-" * 50)
        print(result.read_text(encoding="utf-8"))
    else:
        print("Failed to generate summary")
        sys.exit(1)
